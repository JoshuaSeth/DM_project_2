{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/sethvanderbijl/Coding%20Projects/DM_project_2\n",
      "Installing collected packages: DM-project-2\n",
      "  Attempting uninstall: DM-project-2\n",
      "    Found existing installation: DM-project-2 1.0\n",
      "    Uninstalling DM-project-2-1.0:\n",
      "      Successfully uninstalled DM-project-2-1.0\n",
      "  Running setup.py develop for DM-project-2\n",
      "Successfully installed DM-project-2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/sethvanderbijl/Coding Projects/DM_project_2/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sethvanderbijl/Coding Projects/DM_project_2/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from data.load import load_data\n",
    "from model import RegressionDataset, MultipleRegression\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF with these specific params is cached, returning cached version.\n",
      "No cache for this specific request, start loading base df from disk.\n",
      "saving result of specific arguments to cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 245.26it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data\n",
    "df = load_data(num_rows=100000)\n",
    "test = load_data(mode='our_test', num_rows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# RM redundant features and fill NA\n",
    "y = df['booking_bool']\n",
    "X = df.drop(['booking_bool','click_bool', 'position', 'gross_bookings_usd', 'date_time'], axis=1)\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "X_train, y_train = X, y\n",
    "X_test = test.drop('date_time', axis=1)\n",
    "X_test = X_test.fillna(X.mean()) #Mean of x or mean of x_test?\n",
    "\n",
    "\n",
    "# Split for val data\n",
    "y_test = np.zeros(X_test.shape[0])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=1)\n",
    "\n",
    "# Normalize\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Convert to float\n",
    "y_train, y_test, y_val = y_train.astype(float), y_test.astype(float), y_val.astype(float)\n",
    "\n",
    "train_dataset = RegressionDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "val_dataset = RegressionDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\n",
    "test_dataset = RegressionDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())\n",
    "\n",
    "# Set the params\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.008\n",
    "NUM_FEATURES = X.shape[1]\n",
    "\n",
    "# Save destination\n",
    "prefix = os.path.dirname(os.path.abspath(__file__))\n",
    "best_model_path = prefix+\"/saves/best_new\"+\".pt\"\n",
    "\n",
    "# Initialize datasets as dataloaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1)\n",
    "\n",
    "# I don't have CUDA but maybe you have\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize that model!\n",
    "model = MultipleRegression(NUM_FEATURES)\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adamax(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Keep track of training progress in dict\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "\n",
    "# Save last validation loss to save best model\n",
    "last_val_loss = 9999999999\n",
    "\n",
    "# Let the training beginn\n",
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = model(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch.unsqueeze(1))\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        \n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            y_val_pred = model(X_val_batch)\n",
    "                        \n",
    "            val_loss = criterion(y_val_pred, y_val_batch.unsqueeze(1))\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "        loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "        loss_stats['val'].append(val_epoch_loss/len(val_loader))                              \n",
    "    \n",
    "        print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f}')\n",
    "\n",
    "        # Save the model if val loss is better\n",
    "        if val_epoch_loss/len(val_loader)< last_val_loss:\n",
    "            last_val_loss = val_epoch_loss/len(val_loader)\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "\n",
    "\n",
    "# Visualize loss\n",
    "train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(data=train_val_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\").set_title('Train-Val Loss/Epoch')\n",
    "plt.show()\n",
    "\n",
    "# Test model\n",
    "print(\"starting testing model\")\n",
    "y_pred_list = []\n",
    "\n",
    "# Reload the model that was the best\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X_batch, _ in val_loader: # Originally test-loader\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_pred_list.append(y_test_pred.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "print(\"Predicted and actual\")\n",
    "for y_pred, y_actual in zip(y_pred_list, y_test):\n",
    "    print(y_pred, y_actual)\n",
    "mse = mean_squared_error(y_test, y_pred_list)\n",
    "mae = mean_absolute_error(y_test, y_pred_list)\n",
    "r_square = r2_score(y_test, y_pred_list)\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "print(\"Mean Absolute Error :\",mae)\n",
    "print(\"R^2 :\",r_square)\n",
    "\n",
    "print(\"Values for avg as baseline would be\")\n",
    "y_pred_list = np.full(y_test.size, y_train.mean())\n",
    "mse = mean_squared_error(y_test, y_pred_list)\n",
    "mae = mean_absolute_error(y_test, y_pred_list)\n",
    "r_square = r2_score(y_test, y_pred_list)\n",
    "print(\"Mean Squared Error :\",mse)\n",
    "print(\"Mean Absolute Error :\",mae)\n",
    "print(\"R^2 :\",r_square)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d532e8952df537b50de3bd4c05b7ea6521853c70a8506ea5ed19af6d973f7b4a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
